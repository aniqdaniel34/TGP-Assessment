 Solution Architecture
 
1. AWS Lambda
Purpose: AWS Lambda is a serverless computing service designed to run code in response to events. In this architecture, it is used for data retrieval and processing tasks without the need for provisioning or managing servers.
Functionality: Lambda functions fetch data from the World Bank API. These functions execute specific API queries to retrieve JSON-formatted data. The JSON data is then processed and transformed into CSV format, which is more manageable and compatible with various data processing and storage tools.
Data Flow: Once the data is processed into CSV format, it is automatically stored in an Amazon S3 bucket. This setup allows for efficient and scalable data processing, where Lambda can handle concurrent requests and scale according to the volume of data being processed.
2. Amazon S3
Purpose: Amazon S3 (Simple Storage Service) is a scalable object storage service used for storing and retrieving any amount of data at any time.
Functionality: In this architecture, S3 serves two main purposes:
Data Storage: The processed CSV files generated by AWS Lambda are stored in S3. This storage acts as a central repository for all data fetched from the World Bank API, ensuring data is readily available for further processing and analysis.
Web App Assets: In addition to data storage, S3 stores assets related to the streamlit application, such as the static content, requirement files, and other resources necessary for the application to function.
Integration: S3 acts as the primary data source for the application hosted on EC2, providing the necessary data files for dashboard visualization and other processing tasks.

3. GitHub
Purpose: GitHub is used as the source code repository, facilitating version control and collaboration among developers.
Functionality: 
Repository Management : The application code, written in Python and using the Streamlit framework, is organized and stored in a GitHub repository. This setup enables developers to maintain an organized codebase, where each change is documented, and the evolution of the code is easily trackable. be monitored, and collaboration between team members can be efficiently managed.
Collaboration Tools : GitHub provides collaboration tools like pull requests and code reviews. Developers can propose changes through pull requests, which can then be reviewed and discussed by the team before merging. This process ensures that code quality is maintained and that all team members are aligned with the changes being implemented.
Version Control : The repository allows developers to manage and revert changes through commits and branches. This feature is crucial in ensuring that the development process is flexible, as developers can experiment with different features without affecting the main codebase.
Integration with CI/CD: GitHub integrates seamlessly with AWS CodePipeline, enabling continuous integration and deployment. Any changes committed to the GitHub repository automatically trigger the build and deployment processes via CodePipeline. This integration ensures that the latest version of the application is always available for deployment, streamlining the development and release cycle.

4. AWS CodePipeline
Purpose: AWS CodePipeline automates the entire lifecycle of building, testing, and deploying applications, enhancing efficiency and reducing manual intervention.
Stages:
Source Stage: This stage continuously monitors the GitHub repository for any new commits. Once a change is detected, it triggers the rest of the pipeline, ensuring that updates are processed without delay.
Build/Deploy Stage:
Build: The code is compiled, and necessary dependencies are installed. This stage ensures that the application is ready for deployment.
Deploy: The application is deployed to the designated environment, which could be an EC2 instance or updates to static content stored in S3. In this case, the deployment target is an ss3 bucket.
Version Control and Rollback: AWS CodePipeline supports version control mechanisms that allow teams to roll back to previous versions of the application if issues are detected in a new deployment. This capability is crucial for maintaining application stability and minimizing downtime, ensuring a reliable production environment.

5. Amazon EC2
Purpose: Amazon EC2 (Elastic Compute Cloud) provides scalable computing capacity for hosting applications.
Functionality: 
Application Hosting: The Python application, which utilizes the Streamlit framework to build the web dashboard, is deployed on an EC2 instance. This instance serves the dashboard to end-users via a web interface, ensuring that users can access the application from anywhere.
Synchronization with S3: The EC2 instance is linked to an application stored in an S3 bucket, connected through a CodePipeline to the GitHub repository. A script on the EC2 instance is configured to synchronize with the S3 bucket every five minutes, ensuring the application remains updated with the latest code.
Scalability: EC2 instances can be scaled up or down depending on the application’s demands. This scalability ensures that the system can handle varying levels of traffic and computational requirements, optimizing performance and resource utilization.

6. Amazon CloudWatch
Purpose: Amazon CloudWatch is a monitoring and observability service designed to provide real-time insights into the performance and health of AWS resources.
Functionality:
Monitoring: CloudWatch monitors various performance metrics across the application stack, such as CPU usage, memory consumption, network traffic, and response times. These metrics help maintain optimal application performance.
Alarms and Alerts: CloudWatch can be configured to trigger alarms based on predefined thresholds. For example, if an EC2 instance’s CPU usage exceeds a certain level, CloudWatch can notify the operations team or automatically scale resources to manage the increased load.
Logs: CloudWatch collects and stores logs from different services, facilitating the diagnosis of issues, tracking of errors, and auditing of application behavior. This centralized log management enhances troubleshooting and operational efficiency.



